--- 
layout: post
title: Gesture and Pose Analysis for Parkinson's Disease
permalink: /projects/gesture-pose-analysis/
---

I'm currently in Taipei, Taiwan working as a machine learning researcher at the National Taiwan University. My summer project was to help diagnose and evaluate patients with early-onset Parkinson's Disease using machine learning. Before starting this project, I knew very little about Parkinson's Disease, neurological diseases, or anything about the human brain in general. However, one of the great things about ML / deep learning is its aplicability to numerous scientific fields. With ML, I can quickly learn about a project's specificities, fill knowledge gaps, and get to work. 

The code for this project may be found on my github. Click [here](https://github.com/elliotchen02/PD-analysis) to check it out. 

#### The Problem

According to Wikipedia, [Parkinson's Disease](https://en.wikipedia.org/wiki/Parkinson's_disease) is a degenerative disorder of the neurological system. Diagnosis of early-onset PD can be difficult and begins by evaluation of a patient's medical history and motor symptomns. Multiple symptomns of PD are similar to those of other diseases. Stroke, certain medications, and toxins may cause "secondary Parkinsonism." Faster progression rates, early cognitive dysfunction or postural instability, minimal tremor, or symmetry at onset may indicate another disease rather than PD itself. 

Detection of early-onset PD is based on diagnostic criteria which analyzes a patient's motor skills. Ridigity, tremors, and asymmetry of movements are the most recognized systems. Currently, experts believe roughly 81% of PD diagnoses are accurate based on autopsies. 

The goal of the project was to build a ML model to help diagnose early-onset PD based on video training data of patients. Patients were recorded walking in straight paths, performing unique hand movements, and reading short paragraphs. 

#### Building the Model

The majority of my focus was on hand gesture detection. Patients were recorded performing two unique hand movements. The first, rapidly touching the index and thumb fingers together. Physicians are looking for rapid fatigue or difficulty moving. The second, rapidly twisting the hand with figers outstretched. Again, physicians are looking for fatigue or some movement impediment. 

Video examples are shown below. For privacy reasons, these are my hands. Ignore the fact those gifs make my hands looks massive. 

![Touching Fingers gif](/assets/pics/pd_ntu/touching_fingers.gif)

![Rotating Hands gif](/assets/pics/pd_ntu/rotating_hand.gif)


There was a lack of video data given to the lab by local hospitals and doctors. We only had ~850 videos to work with. Due to the lack of training data, simple ML techniques would suffice to try and make a model. 

The first obstacle was gesture detections for the hands. Luckily, the smart guys at Google already developed MediaPipe, a ML library with gesture models, pose detection, among many others. Using MediaPipe's gesture recognizer, fitting video data and extracting landmarks for the hands was relatively straightforward. There was one major problem with the twisting hand videos. Due to the speed of the turning, and the lack of slow-motion video, MediaPipe struggled to identify the landmarks. To combat this, I used an image sharpening kernel to sharpen straight edges within each frame. This helped the model identify more landmarks, but it still struggled. Room to improve here. I tried using a technique known as HSR filtering, which was a failure. 

Landmarks for hands were framed as shown in the photo below. 

![hand landmarker pic](/assets/pics/pd_ntu/frame_260.jpg)

Each hand was given 21 landmarks. Things brings us to the next step: feature extraction. I decided to extract a few features, most notably the waveforms for the: 
1. distance between index and thumb over time (in the case of the index-thumb movement)
2. rotation speed and period (in the case of the twisting hand)

When plotting these "waveforms" we could identify fluctuations in the movement and potentially identify impediments. If we plot the waveforms, they would look like plot shown below. Peaks are identified through the orange Xs. This particular plot is for the 3rd landmark point: the tip of the thumb. 

![waveform plot](/assets/pics/pd_ntu/waveform.png)

Extracting 21 waveforms for the 21 landmarks allows us to feed our model these datapoints. I must say, I was a bit disappointed that I was not working with a deep learning project - those tend to be more interesting. 

I ran these landmark points through an XGBoost based ML model. Because I'm in a different country, and I only have my M1 macbook, I couldn't run the model on my computer. This was a significant hurdle as I would need to use the lab's server which offered cloud GPUs. This made model training and testing very annoyingly difficult. 

Because of this, I have yet to test the max accuracy of the model. With finer hyperparamater tuning, the model may be quite accurate. I believe the lack of data to begin with is a significant challenge to overcome. 

Overall, it was a good experience working at the National Taiwan University. As someone who only recently began learning about AI, ML, and deep learning, I certainly learned a lot. 



















